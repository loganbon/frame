{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Library Sandbox\n",
    "\n",
    "A temporal dataframe wrapper with parquet caching and concurrent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from frame import Frame, PandasBackend, PolarsBackend, LazyFrame, CacheMode, CacheMissError, ChunkGranularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Create a data-fetching function that accepts `start_dt` and `end_dt` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data function - simulates fetching price data\n",
    "def fetch_prices(start_dt: datetime, end_dt: datetime, ticker: str = \"AAPL\"):\n",
    "    \"\"\"Simulate fetching daily price data for a ticker.\"\"\"\n",
    "    print(f\"Fetching {ticker} from {start_dt.date()} to {end_dt.date()}\")\n",
    "    \n",
    "    dates = pd.date_range(start_dt, end_dt, freq=\"D\")\n",
    "    np.random.seed(hash(ticker) % 2**32)  # Reproducible per ticker\n",
    "    \n",
    "    records = []\n",
    "    base_price = 100 + hash(ticker) % 100\n",
    "    for i, dt in enumerate(dates):\n",
    "        records.append({\n",
    "            \"as_of_date\": dt.to_pydatetime(),\n",
    "            \"id\": ticker,\n",
    "            \"price\": base_price + np.random.randn() * 5 + i * 0.1,\n",
    "            \"volume\": int(1e6 + np.random.randn() * 1e5),\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df.set_index([\"as_of_date\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing cache for fresh start\n",
    "cache_dir = Path(\".frame_cache\")\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "# Create a Frame wrapping the fetch function\n",
    "prices = Frame(\n",
    "    fetch_prices,\n",
    "    {\"ticker\": \"AAPL\"},\n",
    "    backend=\"pandas\",\n",
    "    chunk_granularity=\"month\",  # Cache in monthly chunks (also supports \"day\", \"week\", \"year\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prices.get(datetime(2025, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data for a date range\n",
    "start = datetime(2024, 1, 1)\n",
    "end = datetime(2024, 1, 15)\n",
    "\n",
    "df = prices.get_range(start, end)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch again - this time from cache (no \"Fetching...\" print)\n",
    "df2 = prices.get_range(start, end)\n",
    "print(\"Data fetched from cache!\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for a single date (drops as_of_date from index)\n",
    "single_day = prices.get(datetime(2024, 1, 10))\n",
    "single_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Behavior\n",
    "\n",
    "Data is cached in parquet chunks. Extending the date range only fetches missing chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cache directory\n",
    "cache_files = list(cache_dir.rglob(\"*.parquet\"))\n",
    "for f in cache_files:\n",
    "    print(f\"  {f.relative_to(cache_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the date range - only fetches the new chunk\n",
    "extended = prices.get_range(datetime(2024, 1, 1), datetime(2024, 2, 15))\n",
    "print(f\"\\nTotal rows: {len(extended)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Modes\n",
    "\n",
    "Control cache behavior per-call with the `cache_mode` parameter:\n",
    "\n",
    "| Mode | Name | Read Cache | Write Cache | Fetch Live |\n",
    "|------|------|------------|-------------|------------|\n",
    "| `\"a\"` | Append (default) | Yes | Yes (new data only) | Yes (for missing) |\n",
    "| `\"l\"` | Live | No | No | Always |\n",
    "| `\"r\"` | Read-only | Yes | No | No (raise error if missing) |\n",
    "| `\"w\"` | Write/Refresh | No | Yes (overwrite) | Always |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean cache for fresh demo\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "prices = Frame(fetch_prices, {\"ticker\": \"AAPL\"}, chunk_granularity=\"month\")\n",
    "\n",
    "# Default mode \"a\" (append): read from cache, fetch missing, cache new data\n",
    "start, end = datetime(2024, 1, 1), datetime(2024, 1, 10)\n",
    "df1 = prices.get_range(start, end)  # Fetches and caches\n",
    "print(\"First call (mode='a'): fetched from source, cached\")\n",
    "\n",
    "df2 = prices.get_range(start, end)  # From cache\n",
    "print(\"Second call (mode='a'): served from cache (no fetch)\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live mode \"l\": bypass cache entirely, always fetch fresh\n",
    "df_live = prices.get_range(start, end, cache_mode=\"l\")\n",
    "print(\"Live mode: always fetches fresh data (even if cached)\")\n",
    "df_live.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write mode \"w\": force refresh cache (fetch live and overwrite)\n",
    "df_write = prices.get_range(start, end, cache_mode=\"w\")\n",
    "print(\"Write mode: fetched fresh and overwrote cache\")\n",
    "df_write.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-only mode \"r\": only read from cache, raise error if missing\n",
    "try:\n",
    "    # This date range is not cached, so it will raise CacheMissError\n",
    "    uncached_range = prices.get_range(datetime(2024, 6, 1), datetime(2024, 6, 10), cache_mode=\"r\")\n",
    "except CacheMissError as e:\n",
    "    print(f\"CacheMissError: {e}\")\n",
    "\n",
    "# But this works because Jan 1-10 was cached above\n",
    "df_readonly = prices.get_range(start, end, cache_mode=\"r\")\n",
    "print(\"\\nRead-only mode: served from cache successfully\")\n",
    "df_readonly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Cache Levels\n",
    "\n",
    "Support multiple read-only parent cache directories with one primary write cache.\n",
    "Useful for team/shared caches where you want to read from a shared cache but write to a personal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Create a \"shared\" cache with some data\n",
    "shared_cache = Path(\".frame_cache_shared\")\n",
    "personal_cache = Path(\".frame_cache_personal\")\n",
    "\n",
    "# Clean both caches\n",
    "for c in [shared_cache, personal_cache]:\n",
    "    if c.exists():\n",
    "        shutil.rmtree(c)\n",
    "\n",
    "# First, populate the \"shared\" cache with January data\n",
    "shared_frame = Frame(fetch_prices, {\"ticker\": \"AAPL\"}, cache_dir=shared_cache, chunk_granularity=\"month\")\n",
    "_ = shared_frame.get_range(datetime(2024, 1, 1), datetime(2024, 1, 31))\n",
    "print(\"Shared cache populated with January 2024 data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Frame with hierarchical caching:\n",
    "# - Primary cache (read/write): personal_cache\n",
    "# - Parent cache (read-only): shared_cache\n",
    "hierarchical_frame = Frame(\n",
    "    fetch_prices,\n",
    "    {\"ticker\": \"AAPL\"},\n",
    "    cache_dir=personal_cache,\n",
    "    parent_cache_dirs=[shared_cache],  # Read-only parent caches\n",
    "    chunk_granularity=\"month\",\n",
    ")\n",
    "\n",
    "# Request January data - should come from shared_cache (no fetch!)\n",
    "jan_data = hierarchical_frame.get_range(datetime(2024, 1, 1), datetime(2024, 1, 15))\n",
    "print(\"January: served from shared cache (no fetch)\")\n",
    "print(f\"Got {len(jan_data)} rows\")\n",
    "\n",
    "# Request February data - not in any cache, so fetch and write to personal_cache\n",
    "feb_data = hierarchical_frame.get_range(datetime(2024, 2, 1), datetime(2024, 2, 15))\n",
    "print(f\"\\nFebruary: fetched and cached to personal cache\")\n",
    "print(f\"Got {len(feb_data)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cache structure - February was written to personal cache, not shared\n",
    "print(\"Shared cache files:\")\n",
    "for f in shared_cache.rglob(\"*.parquet\"):\n",
    "    print(f\"  {f.relative_to(shared_cache)}\")\n",
    "\n",
    "print(\"\\nPersonal cache files:\")\n",
    "for f in personal_cache.rglob(\"*.parquet\"):\n",
    "    print(f\"  {f.relative_to(personal_cache)}\")\n",
    "\n",
    "# Cleanup\n",
    "for c in [shared_cache, personal_cache]:\n",
    "    if c.exists():\n",
    "        shutil.rmtree(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Granularity\n",
    "\n",
    "Control how data is chunked in the cache with `chunk_granularity`:\n",
    "\n",
    "| Granularity | Chunk Key Format | Example Path |\n",
    "|-------------|------------------|--------------|\n",
    "| `\"day\"` | `YYYY/MM/DD` | `cache/<key>/2024/01/15.parquet` |\n",
    "| `\"week\"` | `YYYY/W##` | `cache/<key>/2024/W03.parquet` |\n",
    "| `\"month\"` | `YYYY/MM` | `cache/<key>/2024/01.parquet` |\n",
    "| `\"year\"` | `YYYY` | `cache/<key>/2024.parquet` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Day-level granularity (one file per day)\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "daily_frame = Frame(\n",
    "    fetch_prices,\n",
    "    {\"ticker\": \"AAPL\"},\n",
    "    chunk_granularity=\"day\",  # One parquet file per day\n",
    ")\n",
    "\n",
    "_ = daily_frame.get_range(datetime(2024, 3, 1), datetime(2024, 3, 5))\n",
    "\n",
    "print(\"Day granularity cache structure:\")\n",
    "for f in sorted(cache_dir.rglob(\"*.parquet\")):\n",
    "    print(f\"  {f.relative_to(cache_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Year-level granularity (one file per year)\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "yearly_frame = Frame(\n",
    "    fetch_prices,\n",
    "    {\"ticker\": \"AAPL\"},\n",
    "    chunk_granularity=\"year\",  # One parquet file per year\n",
    ")\n",
    "\n",
    "_ = yearly_frame.get_range(datetime(2024, 1, 1), datetime(2024, 12, 31))\n",
    "\n",
    "print(\"Year granularity cache structure:\")\n",
    "for f in sorted(cache_dir.rglob(\"*.parquet\")):\n",
    "    print(f\"  {f.relative_to(cache_dir)}\")\n",
    "\n",
    "# Cleanup\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Frames with Lazy Batching\n",
    "\n",
    "When a Frame's function calls another Frame's `get_range()`, the calls are batched and executed concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean cache for demonstration\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that depends on another Frame\n",
    "def compute_returns(start_dt: datetime, end_dt: datetime, price_frame: Frame):\n",
    "    \"\"\"Compute daily returns from price data.\"\"\"\n",
    "    print(f\"Computing returns from {start_dt.date()} to {end_dt.date()}\")\n",
    "    \n",
    "    # This returns a LazyFrame when called from within another Frame\n",
    "    prices = price_frame.get_range(start_dt, end_dt)\n",
    "    \n",
    "    df = prices.copy()\n",
    "    df[\"return\"] = df[\"price\"].pct_change()\n",
    "    return df\n",
    "\n",
    "# Create the nested Frame\n",
    "prices = Frame(fetch_prices, {\"ticker\": \"AAPL\"})\n",
    "returns = Frame(compute_returns, {\"price_frame\": prices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch returns - the nested price fetch is batched\n",
    "result = returns.get_range(datetime(2024, 1, 1), datetime(2024, 1, 10))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple nested dependencies - all batched together\n",
    "def compute_spread(start_dt: datetime, end_dt: datetime, frame1: Frame, frame2: Frame):\n",
    "    \"\"\"Compute price spread between two tickers.\"\"\"\n",
    "    print(f\"Computing spread from {start_dt.date()} to {end_dt.date()}\")\n",
    "    \n",
    "    # Both of these become LazyFrames, resolved concurrently\n",
    "    p1 = frame1.get_range(start_dt, end_dt)\n",
    "    p2 = frame2.get_range(start_dt, end_dt)\n",
    "    \n",
    "    df = p1.copy()\n",
    "    df[\"spread\"] = p1[\"price\"].values - p2[\"price\"].values\n",
    "    return df\n",
    "\n",
    "aapl = Frame(fetch_prices, {\"ticker\": \"AAPL\"})\n",
    "googl = Frame(fetch_prices, {\"ticker\": \"GOOGL\"})\n",
    "spread = Frame(compute_spread, {\"frame1\": aapl, \"frame2\": googl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both AAPL and GOOGL fetches happen concurrently\n",
    "spread_data = spread.get_range(datetime(2024, 2, 1), datetime(2024, 2, 10))\n",
    "spread_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async API\n",
    "\n",
    "Use `aget_range()` and `aget()` for async operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Clean cache\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch multiple frames concurrently with asyncio.gather\n",
    "aapl = Frame(fetch_prices, {\"ticker\": \"AAPL\"})\n",
    "googl = Frame(fetch_prices, {\"ticker\": \"GOOGL\"})\n",
    "msft = Frame(fetch_prices, {\"ticker\": \"MSFT\"})\n",
    "\n",
    "async def fetch_all():\n",
    "    start = datetime(2024, 4, 1)\n",
    "    end = datetime(2024, 4, 10)\n",
    "    \n",
    "    results = await asyncio.gather(\n",
    "        aapl.aget_range(start, end),\n",
    "        googl.aget_range(start, end),\n",
    "        msft.aget_range(start, end),\n",
    "    )\n",
    "    return results\n",
    "\n",
    "aapl_df, googl_df, msft_df = await fetch_all()\n",
    "print(f\"AAPL: {len(aapl_df)} rows\")\n",
    "print(f\"GOOGL: {len(googl_df)} rows\")\n",
    "print(f\"MSFT: {len(msft_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars Backend\n",
    "\n",
    "Switch to Polars by setting `backend=\"polars\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean cache\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data function that returns Polars DataFrame\n",
    "def fetch_prices_polars(start_dt: datetime, end_dt: datetime, ticker: str = \"AAPL\"):\n",
    "    \"\"\"Simulate fetching daily price data for a ticker (Polars version).\"\"\"\n",
    "    print(f\"Fetching {ticker} from {start_dt.date()} to {end_dt.date()}\")\n",
    "    \n",
    "    dates = pd.date_range(start_dt, end_dt, freq=\"D\")\n",
    "    np.random.seed(hash(ticker) % 2**32)\n",
    "    \n",
    "    base_price = 100 + hash(ticker) % 100\n",
    "    \n",
    "    return pl.DataFrame({\n",
    "        \"as_of_date\": [dt.to_pydatetime() for dt in dates],\n",
    "        \"id\": [ticker] * len(dates),\n",
    "        \"price\": [base_price + np.random.randn() * 5 + i * 0.1 for i in range(len(dates))],\n",
    "        \"volume\": [int(1e6 + np.random.randn() * 1e5) for _ in range(len(dates))],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Frame with Polars backend\n",
    "prices_pl = Frame(\n",
    "    fetch_prices_polars,\n",
    "    {\"ticker\": \"AAPL\"},\n",
    "    backend=\"polars\",\n",
    ")\n",
    "\n",
    "df = prices_pl.get_range(datetime(2024, 5, 1), datetime(2024, 5, 10))\n",
    "print(type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations Layer\n",
    "\n",
    "Operations are Frame-like objects that wrap Frames and apply transformations declaratively.\n",
    "They preserve concurrent data fetching when used alongside Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import operations\n",
    "from frame import Rolling, Shift, Diff, Abs, Pct, Add, Sub, Mul, Div\n",
    "\n",
    "# Clean cache for fresh start\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "# Create a base prices Frame\n",
    "prices = Frame(fetch_prices, {\"ticker\": \"AAPL\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unary Operations\n",
    "\n",
    "Operations that transform a single Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window average (5-day moving average)\n",
    "rolling_avg = Rolling(prices, window=5, func=\"mean\")\n",
    "\n",
    "start = datetime(2024, 1, 1)\n",
    "end = datetime(2024, 1, 15)\n",
    "\n",
    "result = rolling_avg.get_range(start, end)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift/Lag operation - shift data by N periods\n",
    "shifted = Shift(prices, periods=1)\n",
    "shifted.get_range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff - compute difference between consecutive values\n",
    "diff = Diff(prices, periods=1)\n",
    "diff.get_range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pct - percentage change\n",
    "pct_change = Pct(prices, periods=1)\n",
    "pct_change.get_range(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Operations\n",
    "\n",
    "Operations that combine two Frames (or a Frame and a scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second prices Frame\n",
    "googl = Frame(fetch_prices, {\"ticker\": \"GOOGL\"})\n",
    "\n",
    "# Add two Frames element-wise\n",
    "combined = Add(prices, googl)\n",
    "combined.get_range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract: compute price spread between two stocks\n",
    "spread = Sub(prices, googl)\n",
    "spread.get_range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar operations - multiply prices by a constant\n",
    "scaled = Mul(prices, 1.1)  # 10% markup\n",
    "scaled.get_range(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Operations\n",
    "\n",
    "Operations can be chained together - an operation can wrap another operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain: prices -> daily returns -> rolling average of returns\n",
    "returns = Pct(prices, periods=1)\n",
    "smoothed_returns = Rolling(returns, window=3, func=\"mean\")\n",
    "\n",
    "smoothed_returns.get_range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex pipeline: price - lagged_price (equivalent to diff)\n",
    "lagged = Shift(prices, periods=1)\n",
    "daily_change = Sub(prices, lagged)\n",
    "\n",
    "daily_change.get_range(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent Resolution\n",
    "\n",
    "Operations preserve the concurrent fetching behavior. When multiple operations share\n",
    "the same underlying Frame, the data is fetched only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean cache to see fetch behavior\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "# Create multiple operations on the same Frame\n",
    "prices = Frame(fetch_prices, {\"ticker\": \"AAPL\"})\n",
    "rolling_5 = Rolling(prices, window=5)\n",
    "rolling_10 = Rolling(prices, window=10)\n",
    "shifted = Shift(prices, periods=1)\n",
    "\n",
    "# When used in a batch context, prices is fetched only once\n",
    "# and shared by all operations\n",
    "from frame.executor import set_batch_context, reset_batch_context, resolve_batch_sync\n",
    "\n",
    "batch = []\n",
    "token = set_batch_context(batch)\n",
    "\n",
    "try:\n",
    "    # All three operations add their lazy inputs to the batch\n",
    "    lazy_r5 = rolling_5.get_range(start, end)\n",
    "    lazy_r10 = rolling_10.get_range(start, end)\n",
    "    lazy_s = shifted.get_range(start, end)\n",
    "    \n",
    "    print(f\"Items in batch: {len(batch)}\")\n",
    "    print(f\"  - LazyOperations (rolling_5, rolling_10, shifted)\")\n",
    "    print(f\"  - LazyFrames (prices - shared dependency)\")\n",
    "    \n",
    "    # Resolve all at once\n",
    "    resolve_batch_sync(batch)\n",
    "    \n",
    "    print(\"\\nAll resolved successfully!\")\n",
    "finally:\n",
    "    reset_batch_context(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Date Access\n",
    "\n",
    "Operations also support `get()` for single date access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rolling average for a single date\n",
    "rolling_avg = Rolling(prices, window=5)\n",
    "single_day = rolling_avg.get(datetime(2024, 1, 15))\n",
    "single_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Operations\n",
    "\n",
    "Operations support async methods: `aget_range()` and `aget()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async fetch of operation results\n",
    "rolling = Rolling(prices, window=5)\n",
    "\n",
    "async def fetch_rolling():\n",
    "    return await rolling.aget_range(start, end)\n",
    "\n",
    "result = await fetch_rolling()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Memory Cache Layer\n",
    "\n",
    "The Frame library includes a module-level, thread-safe, LRU-based in-memory cache that stores DataFrame chunks after first disk read. This eliminates redundant disk I/O for temporal access patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import memory cache utilities\n",
    "from frame import (\n",
    "    get_memory_cache_stats,\n",
    "    clear_memory_cache,\n",
    "    configure_memory_cache,\n",
    "    CacheStats,\n",
    "    CacheConfig,\n",
    ")\n",
    "\n",
    "# Clean up cache and reset memory cache\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "clear_memory_cache()\n",
    "\n",
    "print(\"Memory cache cleared. Starting fresh demo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CacheManager directly to demonstrate memory cache\n",
    "from frame.cache import CacheManager\n",
    "from frame.backends.pandas import PandasBackend\n",
    "\n",
    "backend = PandasBackend()\n",
    "cache = CacheManager(fetch_prices, {\"ticker\": \"AAPL\"}, backend, cache_dir)\n",
    "\n",
    "# Write a chunk to disk\n",
    "chunk_start = datetime(2024, 6, 1)\n",
    "chunk_end = datetime(2024, 6, 30)\n",
    "\n",
    "df = fetch_prices(chunk_start, chunk_end, ticker=\"AAPL\")\n",
    "cache.write_chunk(df, chunk_start, chunk_end)\n",
    "print(f\"Wrote chunk with {len(df)} rows to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read - memory cache miss, disk read, populates memory cache\n",
    "print(\"First read from disk:\")\n",
    "result1 = cache.read_chunk(chunk_start, chunk_end)\n",
    "\n",
    "stats = get_memory_cache_stats()\n",
    "print(f\"  Memory cache stats: hits={stats.hits}, misses={stats.misses}, entries={stats.current_entries}\")\n",
    "print(f\"  Memory usage: {stats.current_memory_bytes:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second read - memory cache HIT (no disk read!)\n",
    "print(\"Second read (should hit memory cache):\")\n",
    "result2 = cache.read_chunk(chunk_start, chunk_end)\n",
    "\n",
    "stats = get_memory_cache_stats()\n",
    "print(f\"  Memory cache stats: hits={stats.hits}, misses={stats.misses}\")\n",
    "print(f\"  Hit rate: {stats.hit_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure memory cache limits\n",
    "configure_memory_cache(max_entries=256)  # Increase max cached chunks\n",
    "configure_memory_cache(max_memory_bytes=100 * 1024 * 1024)  # 100 MB limit\n",
    "\n",
    "print(\"Cache configured with:\")\n",
    "print(\"  - max_entries: 256\")\n",
    "print(\"  - max_memory_bytes: 100 MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache invalidation on writes - writing to disk invalidates memory cache\n",
    "print(\"Before write - cache entries:\", get_memory_cache_stats().current_entries)\n",
    "\n",
    "# Write new data to the same chunk (simulates data update)\n",
    "df_updated = fetch_prices(chunk_start, chunk_end, ticker=\"AAPL\")\n",
    "cache.write_chunk(df_updated, chunk_start, chunk_end)\n",
    "\n",
    "print(\"After write - cache entries:\", get_memory_cache_stats().current_entries)\n",
    "print(\"  -> Memory cache invalidated on write (write-through policy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the entire memory cache\n",
    "# Read to populate cache again first\n",
    "_ = cache.read_chunk(chunk_start, chunk_end)\n",
    "print(\"After read - entries:\", get_memory_cache_stats().current_entries)\n",
    "\n",
    "# Now clear\n",
    "cleared_count = clear_memory_cache()\n",
    "print(f\"Cleared {cleared_count} entries from memory cache\")\n",
    "print(\"After clear - entries:\", get_memory_cache_stats().current_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cache uses composite keys: (path, columns, filters)\n",
    "# Different column selections create separate cache entries\n",
    "clear_memory_cache()\n",
    "\n",
    "# Read all columns\n",
    "df_all = cache.read_chunk(chunk_start, chunk_end)\n",
    "print(f\"Read all columns - entries: {get_memory_cache_stats().current_entries}\")\n",
    "\n",
    "# Read only specific columns (different cache key!)\n",
    "df_price_only = cache.read_chunk(chunk_start, chunk_end, columns=[\"price\"])\n",
    "print(f\"Read 'price' column only - entries: {get_memory_cache_stats().current_entries}\")\n",
    "\n",
    "# Read with different columns (another cache key)\n",
    "df_volume_only = cache.read_chunk(chunk_start, chunk_end, columns=[\"volume\"])\n",
    "print(f\"Read 'volume' column only - entries: {get_memory_cache_stats().current_entries}\")\n",
    "\n",
    "print(\"\\nEach unique (path, columns, filters) combo gets its own cache entry!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable memory cache if needed (e.g., for debugging or memory-constrained environments)\n",
    "configure_memory_cache(enabled=False)\n",
    "print(\"Memory cache disabled\")\n",
    "\n",
    "# Reads now always go to disk\n",
    "_ = cache.read_chunk(chunk_start, chunk_end)\n",
    "print(f\"After read with cache disabled - entries: {get_memory_cache_stats().current_entries}\")\n",
    "\n",
    "# Re-enable for normal operation\n",
    "configure_memory_cache(enabled=True)\n",
    "print(\"\\nMemory cache re-enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Cache API Summary\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `get_memory_cache_stats()` | Get current cache statistics (hits, misses, entries, memory) |\n",
    "| `clear_memory_cache()` | Clear all entries, returns count cleared |\n",
    "| `configure_memory_cache(**kwargs)` | Configure cache settings |\n",
    "\n",
    "**Configuration options:**\n",
    "- `enabled` (bool): Enable/disable cache (default: True)\n",
    "- `max_entries` (int): Max cached chunks, 0 = unlimited (default: 128)\n",
    "- `max_memory_bytes` (int): Max memory usage, 0 = unlimited (default: 0)\n",
    "\n",
    "**Key features:**\n",
    "- Thread-safe with fine-grained locking\n",
    "- LRU eviction when limits exceeded\n",
    "- Composite keys: (path, columns, filters)\n",
    "- Write-through invalidation on disk writes\n",
    "- Works with both pandas and polars DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cache directory when done experimenting\n",
    "if cache_dir.exists():\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(\"Cache cleaned up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
